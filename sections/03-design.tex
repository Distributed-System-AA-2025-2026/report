\section{Design}\label{design}

This chapter explains the strategies used to meet the requirements identified in the analysis.

\subsection{Architecture}\label{architecture}

\subsubsection{Hub Server}

The Hub Server adopts a \textbf{peer-to-peer overlay} architectural style, where each hub instance is both a client and a server within the gossip protocol.
No hub acts as a coordinator or leader: every instance maintains its own local copy of the cluster state and makes autonomous decisions.

This choice is motivated by the AP orientation identified in the requirements.
A leader-based architecture (e.g., Raft, Paxos) would provide strong consistency but would sacrifice availability during leader elections or network partitions.
Since the Hub's primary role is matchmaking (returning a joinable room to clients), temporary inconsistencies between hubs (e.g., two hubs briefly disagreeing on a room's player count) are tolerable and self-correcting via gossip convergence.

The client-facing layer uses a standard \textbf{request-response} pattern over HTTP (via FastAPI), decoupled from the internal gossip communication.
This separates the concerns of client interaction (synchronous, HTTP) from cluster coordination (asynchronous, UDP gossip).

\subsubsection{Room Server}

The Room Server adopts a \textbf{Client-Server (Authoritative Server)} architectural style. 
The Room Server acts as the centralized authority for a specific game match. 
Clients do not communicate with each other directly; instead, they send their intentions (inputs) to the Room Server, which computes the true state of the world and broadcasts it back.

This choice is strictly motivated by the \textbf{CP-orientation} required for game logic. 
In a fast-paced game like Bomberman, concurrent actions (e.g., two players attempting to place a bomb in the same spot simultaneously) must be resolved deterministically.
An authoritative server ensures a single source of truth, preventing desynchronization and mitigating client-side cheating.

\subsection{Infrastructure}\label{infrastructure}

\subsubsection{Hub Server}

The infrastructure comprises the following components:

\paragraph{Hub Instances.}
Each hub runs as a Kubernetes \texttt{StatefulSet} pod.
The StatefulSet guarantees stable hostnames (e.g., \texttt{hub-0...}, \texttt{hub-1...}), which are used to derive the hub index and to construct DNS-based \texttt{ServerReference}\footnote{A class that is used to encapsule Server ip and port.} addresses for gossip communication.
Each hub pod exposes two network interfaces:
\begin{itemize}
  \item \textbf{HTTP API} (port 8000, via FastAPI/Uvicorn): serves client matchmaking requests, room lifecycle callbacks from Room Servers (\texttt{/room/\{id\}/start}, \texttt{/room/\{id\}/close}), health/readiness probes, and a debug endpoint.
  \item \textbf{UDP gossip socket} (configurable via \texttt{GOSSIP\_PORT}\footnote{Port 9000 is used in this project}): used for peer-to-peer gossip protocol communication, bound to \texttt{0.0.0.0} to accept messages from any peer.
\end{itemize}

\paragraph{Room Instances.}
Each Room Server runs as an independent Kubernetes pod, created and managed by the hub's \texttt{RoomManager}.
Room pods are exposed via \texttt{NodePort} services, allowing external clients to connect directly to the game server.
An internal \texttt{ClusterIP} service is also created for each room, used by the \texttt{RoomHealthMonitor} to perform HTTP health checks on port 8080.

\paragraph{Service Discovery.}
Components discover each other through two mechanisms depending on the deployment mode:
\begin{itemize}
  \item \textbf{Manual mode}: peers are addressed on \texttt{127.0.0.1} with port-based indexing (\texttt{9000 + peer\_index}). Suitable for local development and testing, please note that without this mode the project it's deployable in local without k8s.
  \item \textbf{Kubernetes mode}: peers are addressed via DNS using the pattern \texttt{hub-\{index\}.hub-service.bomberman.svc.cluster.local}, resolved by the Kubernetes DNS service. The \texttt{HUB\_SERVICE\_NAME} and \texttt{K8S\_NAMESPACE} environment variables configure this resolution.
\end{itemize}

Room Servers are discovered by the hub via the Kubernetes API (\texttt{CoreV1Api}), which is also used to create, delete, and inspect room pods and services.

\paragraph{Network Topology.}
All components run within the same Kubernetes cluster (same network namespace).
Hub-to-hub gossip uses UDP within the cluster network.
Client-to-hub communication enters through an external ingress.
Client-to-room communication uses NodePort services, exposing game traffic directly on cluster node ports.

\subsubsection{Room Server}

The infrastructure for the Room tier consists of dynamically provisioned compute units:

\paragraph{Room Instances.}
Each Room Server is a self-contained process. In the Kubernetes deployment mode, it runs as an independent \texttt{Pod} spawned dynamically by the Hub Server's \texttt{RoomManager}. 
Each Room Pod is isolated, meaning the computational load of one match does not impact others.

\paragraph{Network Interfaces.}
Each Room Server exposes two distinct network interfaces:
\begin{itemize}
  \item \textbf{Game Port (TCP):} A socket bound to an IP address, exposed externally via a Kubernetes \texttt{NodePort} Service. 
  This allows external game clients to bypass the Hub tier and establish persistent, TCP connections directly to the node hosting the Room Server.
  \item \textbf{Management API (HTTP):} An internal HTTP server exposed via a \texttt{ClusterIP} service. 
  This is used exclusively by the Hub Server for lifecycle management and health monitoring (\texttt{GET /status}).
\end{itemize}

\paragraph{Discovery.}
Clients discover the Room Server via the Hub: the matchmaking response provides the exact external IP and \texttt{NodePort} to connect to. 
The Hub discovers the Room Server's internal IP via the Kubernetes API upon Pod creation.

\subsection{Modelling}\label{modelling}

\subsubsection{Hub Server}

\paragraph{Domain Entities.}

\begin{itemize}
  \item \textbf{HubPeer}: represents a known hub instance in the cluster. Each peer has an \texttt{index} (derived from hostname), a \texttt{ServerReference} (address + port), a \texttt{status} (\texttt{alive}, \texttt{suspected}, \texttt{dead}), a monotonically increasing \texttt{heartbeat} counter (used for duplicate detection), and a \texttt{last\_seen} timestamp (used for failure detection).
  \item \textbf{Room}: represents a game session. Each room has a \texttt{room\_id}, an \texttt{owner\_hub\_index} (the hub that created it), a \texttt{status} (following the lifecycle \texttt{DORMANT} $\rightarrow$ \texttt{ACTIVE} $\rightarrow$ \texttt{PLAYING} $\rightarrow$ \texttt{CLOSED/DORMANT}), a \texttt{player\_count} with a configurable \texttt{max\_players} cap, and network details (\texttt{external\_port}, \texttt{internal\_service}).
  \item \textbf{HubState}: the aggregate root that holds the local view of the cluster. It contains a list of \texttt{HubPeer} instances (indexed by peer index, with \texttt{None} gaps for unknown indices) and a dictionary of \texttt{Room} instances (keyed by \texttt{room\_id}). All access is synchronized via a \texttt{threading.RLock}.
  \item \textbf{ServerReference}: a value object representing a network endpoint (address + port).
  \item \textbf{RoomStatus}: an enumeration defining the room lifecycle states: \texttt{DORMANT} (created but not accepting players), \texttt{ACTIVE} (waiting for players), \texttt{PLAYING} (game in progress), and \texttt{CLOSED} (ended, pending cleanup). This enum drives the joinability logic and state transition rules throughout the system.
  \item \textbf{GossipMessage}: the protocol envelope for all inter-hub communication. Each message carries a \texttt{nonce} (monotonic per origin), \texttt{origin} and \texttt{forwarded\_by} indices, a \texttt{timestamp}, an \texttt{event\_type} discriminator, and exactly one \texttt{oneof payload} matching the event type. Defined as a Protocol Buffers message.
  \item \textbf{Event Payloads}: a set of value objects, one per event type, carrying event-specific data. Examples include \texttt{RoomActivatedPayload} (room\_id, owner\_hub, external\_port, external\_address), \texttt{PeerJoinPayload} (joining\_peer index), and \texttt{RoomPlayerJoined} (room\_id). Each payload is a Protocol Buffers message nested within the \texttt{GossipMessage} envelope.
\end{itemize}

\paragraph{Domain Events.}

The gossip protocol defines two categories of events, encoded as \texttt{EventType} in the Protocol Buffers schema:

\begin{itemize}
  \item \textbf{Peer lifecycle events}: \texttt{PEER\_JOIN} (a hub announces itself), \texttt{PEER\_LEAVE} (graceful shutdown), \texttt{PEER\_ALIVE} (liveness declaration in response to suspicion), \texttt{PEER\_SUSPICIOUS} (a hub suspects another is unresponsive), \texttt{PEER\_DEAD} (a hub declares another dead after timeout).
  \item \textbf{Room lifecycle events}: \texttt{ROOM\_ACTIVATED} (a room starts accepting players, includes owner hub, external port, and address), \texttt{ROOM\_PLAYER\_JOINED} (player count increment), \texttt{ROOM\_STARTED} (game begins, room no longer joinable), \texttt{ROOM\_CLOSED} (game ended, room returns to dormant).
\end{itemize}

\paragraph{Messages.}

All gossip messages share a common envelope (\texttt{GossipMessage}) containing:
\begin{itemize}
  \item \texttt{nonce}: monotonically increasing counter per origin hub, used for duplicate detection and message ordering.
  \item \texttt{origin}: the index of the hub that originally created the message.
  \item \texttt{forwarded\_by}: the index of the hub that last forwarded the message (changes at each hop).
  \item \texttt{timestamp}: message creation time.
  \item \texttt{event\_type}: enum discriminator for the payload.
  \item One of payload field carrying the event-specific data.
\end{itemize}

\paragraph{System State.}

Each hub maintains:
\begin{itemize}
  \item A \textbf{peer list}: all known hub instances with their status, heartbeat counter, and last seen timestamp. This is the hub's local membership view.
  \item A \textbf{room registry}: all known rooms (both local and remote) with their lifecycle status and player count. This is the hub's local view of available game sessions.
  \item A \textbf{nonce counter}: the last used nonce for outgoing messages, ensuring monotonicity.
\end{itemize}

\subsubsection{Room Server}

\paragraph{Domain Entities.}
\begin{itemize}
  \item \textbf{Game Engine}: The aggregate root representing the entire match state. 
  It contains the grid, the list of players, active bombs, and power-ups.
  \item \textbf{Player}: Represents an in-game player, tracking coordinates player's ID, position, alive/dead status, and if they are the owner of a bomb.
  \item \textbf{Bomb}: Represents a bomb placed on the grid, tracking its position, owner, explosion timer, and blast range.
\end{itemize}

\paragraph{Entity Mapping.}
The authoritative \texttt{Game Engine} resides exclusively in the Room Server's memory. 
Clients only hold a string-based representation of this model. 
When a domain event occurs (e.g., a bomb explodes), the Room Server calculates the outcome (destroyed walls, killed players) and broadcasts a new snapshot to all clients.

\paragraph{Domain Events and Messages.}
Communication between Client and Room Server relies on compact Protocol Buffers exchanged over a TCP stream.
\begin{itemize}
  \item \textbf{Commands (Client $\rightarrow$ Server)}: Represent player intentions (actions).
  \item \textbf{State Updates (Server $\rightarrow$ Client)}: The server periodically broadcasts a snapshot message, ensuring clients continually synchronize with the authoritative state.
\end{itemize}

\subsection{Interaction}\label{interaction}

\subsubsection{Hub Server}

\paragraph{Client $\rightarrow$ Hub (HTTP).}
The client sends a \texttt{POST /matchmaking} request.
The hub consults its local \texttt{HubState} for a joinable room (status \texttt{ACTIVE}, \texttt{player\_count < max\_players}).
If found, the hub increments the player count, broadcasts a \texttt{ROOM\_PLAYER\_JOINED} gossip message, and returns the room's address and port.
If no joinable room exists, the hub asks its \texttt{RoomManager} to activate a dormant room (or create a new one in K8s mode); upon success, it broadcasts \texttt{ROOM\_ACTIVATED} and returns the new room's details.
If activation fails, the hub returns HTTP 503.

\paragraph{Room $\rightarrow$ Hub (HTTP callbacks).}
The Room Server notifies the hub of lifecycle transitions:
\begin{itemize}
  \item \texttt{POST /room/\{id\}/start}: triggers a \texttt{ROOM\_STARTED} gossip broadcast and sets the room status to \texttt{PLAYING}.
  \item \texttt{POST /room/\{id\}/close}: triggers a \texttt{ROOM\_CLOSED} gossip broadcast and sets the room status to \texttt{DORMANT}.
\end{itemize}

\paragraph{Hub $\leftrightarrow$ Hub (UDP Gossip).}
When a hub receives a gossip message via \texttt{HubSocketHandler}, the following pipeline executes:
\begin{enumerate}
  \item \textbf{Resolve sender}: in K8s mode, the sender reference is recalculated from the \texttt{forwarded\_by} index (because UDP source addresses may differ from the pod's DNS name).
  \item \textbf{Ensure peers exist}: the origin and forwarder are added to the peer list if not already known.
  \item \textbf{Heartbeat check}: the message's nonce is compared against the stored heartbeat for the origin peer. If the nonce is not newer, the message is a duplicate and is discarded. The forwarder is marked as alive regardless.
  \item \textbf{Process payload}: the event-specific handler is invoked (e.g., \texttt{\_handle\_room\_activated} adds a room to local state).
  \item \textbf{Forward}: the message is forwarded to a random subset of alive peers (bounded by \texttt{HUB\_FANOUT}), with the \texttt{forwarded\_by} field updated to the current hub's index.
\end{enumerate}

\paragraph{Hub $\rightarrow$ Room (Health Check).}
The \texttt{RoomHealthMonitor} periodically sends \texttt{GET /status} HTTP requests to each \texttt{ACTIVE} room's internal service.
If the room does not respond with status \texttt{WAITING\_FOR\_PLAYERS}, the hub marks it as unhealthy: local rooms are transitioned to \texttt{PLAYING} (with a gossip broadcast), remote rooms are removed from local state.

\subsubsection{Room Server}

\paragraph{Client $\leftrightarrow$ Room (TCP Stream).}
Once a client connects to the Room Server, the interaction follows an \textbf{Asynchronous Input / Synchronous Broadcast} pattern:
\begin{enumerate}
  \item \textbf{Input Collection}: Clients send actions messages at varying frequencies based on user input. 
  The Room Server asynchronously receives these and places them into a thread-safe input queue.
  \item \textbf{Tick-Based Broadcast}: The Room Server does not respond to inputs immediately.
  Instead, at a fixed frequency (e.g., 10 Hz), a dedicated game loop thread processes all queued inputs, updates the game state, and broadcasts the resulting snapshot to all connected clients simultaneously.
\end{enumerate}

\paragraph{Room $\rightarrow$ Hub (HTTP).}
The Room Server uses a \textbf{Webhook} pattern to notify the Hub of major lifecycle shifts, sending HTTP \texttt{POST} requests to the Hub when the required number of players has connected (\texttt{/start}) and when the match ends (\texttt{/close}).

\subsection{Behaviour}\label{behaviour}

\subsubsection{Hub Server}

\paragraph{HubServer (Stateful, Coordinator).}
The \texttt{HubServer} class is the central coordinator.
On initialization, it creates and starts all subsystems (socket handler, failure detector, peer discovery monitor, room manager, room health monitor).
It reacts to incoming gossip messages via callback dispatch (\texttt{\_on\_gossip\_message}), to failure detector callbacks (\texttt{\_on\_peer\_suspicious}, \texttt{\_on\_peer\_dead}), to peer discovery triggers (\texttt{\_discovery\_peers}), and to room health alerts (\texttt{\_on\_room\_unhealthy}).
On shutdown (\texttt{stop()}), it broadcasts a \texttt{PEER\_LEAVE} message and cleanly stops all subsystems.

\paragraph{HubState (Stateful, Passive).}
\texttt{HubState} is a thread-safe container.
It does not initiate any actions; it only responds to queries and mutations from the \texttt{HubServer} and background threads.
All methods acquire the internal \texttt{RLock} before accessing shared data.
Notable behavior: \texttt{execute\_heartbeat\_check} implements the duplicate detection logic. It returns \texttt{True} only if the received nonce is strictly greater than the stored one, or if a dead peer is returning (resurrection).
A special case prevents propagation of \texttt{PEER\_LEAVE} messages for already-dead peers.

\paragraph{FailureDetector (Stateful, Active).}
Runs a background thread that periodically iterates over all known peers (excluding self).
For each peer, it computes the silence duration (\texttt{now - last\_seen}).
The state machine transitions are:
\begin{itemize}
  \item \texttt{alive} $\rightarrow{\text{silence} > \texttt{SUSPECT\_TIMEOUT}}$ \texttt{suspected} (triggers \texttt{on\_peer\_suspected} callback)
  \item \texttt{alive} or \texttt{suspected} $\rightarrow{\text{silence} > \texttt{DEAD\_TIMEOUT}}$ \texttt{dead} (triggers \texttt{on\_peer\_dead} callback)
  \item \texttt{dead} $\rightarrow$ no further transitions (peer is ignored)
\end{itemize}
Note: an alive peer can transition directly to dead if the silence exceeds \texttt{DEAD\_TIMEOUT} without an intermediate suspected check.

\paragraph{PeerDiscoveryMonitor (Stateful, Active).}
Runs a background thread that periodically counts alive peers (excluding self).
If the count is below the configured fanout, it triggers the \texttt{\_discovery\_peers} callback, which sends a \texttt{PEER\_JOIN} message to a randomly selected peer.
This ensures the hub continuously attempts to maintain a sufficient peer count for effective gossip dissemination.

\paragraph{RoomManager (Stateful, Passive).}
Manages the local pool of room instances.
In K8s mode, it creates pods and NodePort services via the Kubernetes API.
When \texttt{activate\_room()} is called and no dormant room is available, the K8s implementation dynamically creates a new room (the local implementation cannot).
On startup, K8s mode recovers existing room pods from a previous run (\texttt{\_recover\_existing\_rooms}).

\paragraph{RoomHealthMonitor (Stateful, Active).}
Runs a background thread that periodically checks all \texttt{ACTIVE} rooms via HTTP.
It only checks rooms with a known \texttt{internal\_service} (skipping remote rooms without internal addressing).
When a room fails the health check, the behavior depends on ownership: local rooms are transitioned to \texttt{PLAYING} (assuming the game has started), remote rooms are removed from state (assuming the remote hub will handle it).

\paragraph{HubSocketHandler (Stateless, Active).}
Manages the UDP socket.
The listener thread spawns a new thread for each received datagram (to avoid blocking the receive loop during message processing).
Message parsing (protobuf deserialization) and callback invocation happen in the handler thread.
Send operations are fire-and-forget: errors (DNS resolution failure, socket errors) are logged but do not propagate.

\subsubsection{Room Server}

\paragraph{GameLoop (Stateful, Active).}
The core behavior of the Room Server is driven by a single, stateful \texttt{GameLoop} thread. 
This thread dictates the passage of time in the game. It is responsible for popping inputs from the network queues, updating the \texttt{Game Engine} (e.g., decrementing bomb timers, moving players), and broadcasting the new state. It strictly enforces a \texttt{sleep} interval at the end of each cycle to maintain a consistent tick rate.

\paragraph{ClientHandler (Stateful, Passive).}
For every connected client, the Room Server spawns a \texttt{ClientHandler} thread. 
Its sole behavior is to block on the TCP socket, deserialize incoming Protocol Buffer messages, and append them to the main \texttt{GameLoop}'s input queue. 
It does not update the game state directly, preventing concurrency anomalies.

\subsection{Data and Consistency Issues}\label{data-and-consistency-issues}

\subsubsection{Hub Server}

\paragraph{Stored Data.}
The Hub Server does not use persistent storage.
All state is held in memory within the \texttt{HubState} object.
If a hub crashes and restarts, it reconstructs its peer list through gossip (receiving \texttt{PEER\_JOIN}/\texttt{PEER\_ALIVE} messages from other hubs) and its room list through the K8s API (\texttt{\_recover\_existing\_rooms} queries existing pods).
This design choice eliminates the need for a database and simplifies the deployment, at the cost of a brief inconsistency window during recovery.

\paragraph{Shared Data.}
The peer list and room registry are conceptually shared across all hubs, but each hub maintains its own copy.
There is no shared database or distributed data structure.
Consistency is achieved through gossip propagation: when a hub modifies its local state (e.g., activating a room), it broadcasts the change to the cluster.

\paragraph{Consistency Model.}
The system provides \textbf{eventual consistency}.
After a state change originates at one hub, the gossip protocol propagates it to all alive hubs through fanout-bounded forwarding.
The convergence time depends on the cluster size, fanout parameter, and gossip round interval.
Duplicate messages are discarded via per-origin nonce tracking, ensuring idempotent processing.

Possible temporary inconsistencies include:
\begin{itemize}
  \item Two hubs simultaneously activating a room for the same matchmaking request (if the \texttt{ROOM\_ACTIVATED} message hasn't propagated yet). This results in two active rooms, which is acceptable since it simply increases capacity.
  \item Player count divergence: if a \texttt{ROOM\_PLAYER\_JOINED} message is lost, some hubs may have a stale count. This is tolerable because the Room Server itself is the authoritative source for whether a game can start.
\end{itemize}

\paragraph{Concurrent Access.}
All access to \texttt{HubState} is serialized via a \texttt{threading.RLock} (reentrant lock).
This is necessary because multiple threads concurrently access the state: the HTTP request handler thread, the gossip message handler threads (one per received datagram), the failure detector thread, the peer discovery monitor thread, and the room health monitor thread.
The reentrant property is used because some methods (e.g., \texttt{mark\_forward\_peer\_as\_alive}) call other locked methods internally (e.g., \texttt{add\_peer}).

\subsubsection{Room Server}

\paragraph{Stored Data.}
The Room Server persists the game state to disk. 
The \texttt{Game Engine} class, which encapsulates the entire match state, is periodically serialized and saved as a Python pickle file (\texttt{.pkl}). 
While active gameplay reads and writes to RAM for low-latency performance, dumping the state to a \texttt{.pkl} file provides a persistent snapshot of the game. 
This file-based storage acts as a localized persistence mechanism, allowing the server to maintain a recoverable state of the match.

\paragraph{Concurrent Access and Consistency.}
The Room Server enforces \textbf{Strong Consistency}. 
Since multiple \texttt{ClientHandler} threads receive data concurrently, all shared access to the game state is serialized. 
Instead of relying on fine-grained locks which could cause deadlocks, the architecture uses a \textbf{Single-Writer principle}: the \texttt{GameLoop} thread is the \textit{only} entity allowed to mutate the \texttt{Game Engine} and to safely execute the disk I/O operations to save the \texttt{.pkl} file. 
Concurrent writes (client inputs) are marshaled via thread-safe queues, ensuring the state is never pickled while partially modified.

\paragraph{Shared Data.}
The authoritative server model ensures that conflicting requests are resolved based on the order they are pulled from the queue during the tick.

\subsection{Fault-Tolerance}\label{fault-tolerance}

\subsubsection{Hub Server}

\paragraph{Data Replication.}
Each hub holds a full replica of the cluster state (peer list + room registry).
Replication is achieved through the gossip protocol: every state-changing event is broadcast to the cluster.
This is a form of \textbf{optimistic replication}, where each replica applies updates locally without coordinating with others, and conflicts are resolved by convergence (last-write-wins based on nonce ordering).

\paragraph{Heartbeating and Failure Detection.}
The \texttt{FailureDetector} implements a two-phase timeout-based scheme:
\begin{itemize}
  \item \textbf{Suspect phase} (\texttt{SUSPECT\_TIMEOUT}, default 5 seconds): if a peer has not been heard from within this interval, it is marked as \texttt{suspected}. A \texttt{PEER\_SUSPICIOUS} message is broadcast, giving the suspected peer a chance to respond with a \texttt{PEER\_ALIVE} declaration.
  \item \textbf{Dead phase} (\texttt{DEAD\_TIMEOUT}, default 20 seconds): if the silence continues, the peer is marked as \texttt{dead}. A \texttt{PEER\_DEAD} message is broadcast, and the peer is removed from the active peer list.
\end{itemize}

The two-phase design avoids premature removal: transient network delays or temporary processing stalls trigger suspicion (which is reversible) rather than immediate removal.
The ``alive'' declaration mechanism allows a falsely suspected peer to rehabilitate itself.

Gossip messages themselves serve as implicit heartbeats: any message received from a peer (regardless of type) updates the forwarder's \texttt{last\_seen} timestamp via \texttt{mark\_forward\_peer\_as\_alive}.

\paragraph{Peer Recovery.}
A dead peer can rejoin the cluster.
In \texttt{execute\_heartbeat\_check}, if a message arrives from a peer currently marked as \texttt{dead}, the peer's status is reset to \texttt{alive} and the message is processed normally.
This allows hubs that crashed and restarted to reintegrate without manual intervention.

\paragraph{Room Recovery.}
In Kubernetes mode, if a hub restarts, the \texttt{K8sRoomManager} queries the Kubernetes API for existing room pods labeled with the hub's index.
Running or pending pods are recovered into the local room registry, avoiding orphaned rooms.

\paragraph{Error Handling.}
\begin{itemize}
  \item \textbf{Network errors}: UDP send failures (DNS resolution, socket errors) are logged but do not crash the hub. The gossip protocol's redundant forwarding paths compensate for individual message losses.
  \item \textbf{Malformed messages}: protobuf deserialization errors are caught in \texttt{HubSocketHandler.\_handle\_message} and logged, preventing invalid data from corrupting hub state.
  \item \textbf{Room health failures}: rooms that fail HTTP health checks are handled gracefully (local rooms transitioned, remote rooms removed from state).
  \item \textbf{Kubernetes API failures}: pod/service creation errors are caught and logged, allowing the hub to continue operating with a reduced room pool.
\end{itemize}

\subsubsection{Room Server}

\paragraph{Local Checkpointing.}
At the Room tier, there is no active state replication across multiple nodes. 
A match is entirely isolated to a single Pod. However, to mitigate complete data loss, the Room Server implements \textbf{local checkpointing}. 
By periodically serializing and dumping the \texttt{Game Engine} state to a \texttt{.pkl} file, the system creates a persistent snapshot of the match without the overhead of network replication.

\paragraph{Error Handling and Disconnections.}
\begin{itemize}
  \item \textbf{Server Failure \& Recovery}: If a Room Server process crashes unexpectedly, the match state is not permanently lost. Upon restart, the server can deserialize the latest \texttt{.pkl} snapshot to restore the \texttt{Game Engine}. 
  This allows the match to resume from the last saved checkpoint, losing only the transient inputs that occurred after the last disk write. 
  The checkpoint is considered a best-effort recovery mechanism, as it does not guarantee zero data loss, but it significantly reduces the impact of server failures.
  The saved state is considered invalid if it is older than a configured threshold (e.g., 30 seconds), in which case the server starts a new match instead of attempting recovery.
  
  \item \textbf{Client Failure}: The \texttt{ClientHandler} actively monitors the TCP socket. 
  If a socket throws an \texttt{EOFError} or \texttt{ConnectionResetError}, the client is considered disconnected. The Room Server updates the player's status within the \texttt{Game Engine} and continues the match for the remaining players.
  
  \item \textbf{Graceful Reconnection}: Because the state is preserved in the \texttt{Game Engine} (and backed up in the \texttt{.pkl}), the server retains the disconnected player's \textit{User ID} and avatar state. 
  If the player re-establishes the TCP connection within a bounded time window, they can reclaim their previous state (position, alive/dead status) and rejoin the match seamlessly.
\end{itemize}

\subsection{Availability}\label{availability}

\subsubsection{Hub Server}

\paragraph{No Caching.}
The Hub Server does not implement caching.
The local \texttt{HubState} already serves as an in-memory replica of the cluster state, making additional caching unnecessary.
Every matchmaking request is served from the local state without external lookups.

\paragraph{Load Distribution.}
The system does not use a traditional load balancer between hubs.
Instead, a Kubernetes Service distributes incoming HTTP requests across hub pods.
Since each hub holds a full replica of the room registry, any hub can serve any matchmaking request independently, making the distribution stateless and trivially parallelizable.

For gossip traffic, the fanout-bounded forwarding naturally distributes message propagation load: each hub forwards to at most \texttt{HUB\_FANOUT} peers, randomly selected, preventing any single hub from becoming a bottleneck.

\paragraph{Network Partitioning.}
Under a network partition, the system behaves as follows (consistent with its AP orientation):
\begin{itemize}
  \item Each partition continues to operate independently. Hubs in each partition serve matchmaking requests from their local state.
  \item Hubs in different partitions will diverge: room activations and player joins on one side will not be visible on the other.
  \item The failure detector will eventually mark unreachable peers as dead.
  \item When the partition heals, hubs rediscover each other through the \texttt{PeerDiscoveryMonitor}. Dead peers that send new messages are automatically resurrected via the heartbeat check logic. Room state converges as new gossip events propagate.
  \item The main consequence is that during the partition, the same room might be assigned to clients from both sides, or duplicate rooms might be activated. This is acceptable because the Room Server itself enforces the player cap, and excess rooms simply remain unused.
\end{itemize}

\subsubsection{Room Server}

\paragraph{Network Partitioning.}
Because the Room Server operates on a CP (Consistency/Partition Tolerance) model, it favors consistency over availability.
If a network partition isolates a client from the Room Server:
\begin{itemize}
  \item The server will stop receiving inputs from that client, leaving their player idle.
  \item Once the TCP's timeout threshold is reached, the server drops the connection to maintain state integrity.
\end{itemize}

\subsection{Security}\label{security}

\subsubsection{Hub Server}

The Hub Server currently operates under a \textbf{trusted environment assumption}: all components run within a private Kubernetes cluster, and no external actors have direct access to the gossip protocol or the hub's HTTP management endpoints.

\paragraph{Authentication and Authorization.}
No authentication is implemented on any endpoint.
The matchmaking endpoint, room lifecycle callbacks, and debug endpoint are all unauthenticated.
In a production deployment, the following measures could be added:
\begin{itemize}
  \item API key or token-based authentication for room-to-hub callbacks (\texttt{/room/\{id\}/start}, \texttt{/room/\{id\}/close}).
  \item Network-level access control via Kubernetes \texttt{NetworkPolicy} to restrict gossip traffic to hub pods only.
  \item Disabling or restricting access to the \texttt{/debug/} endpoint, which exposes internal cluster state.
\end{itemize}

\paragraph{Gossip Security.}
Gossip messages are neither authenticated nor encrypted.
A malicious actor with access to the cluster network could inject forged gossip messages (e.g., false \texttt{PEER\_DEAD} declarations to destabilize the cluster).
However, since the system assumes an internal Kubernetes network, this risk is accepted.
Input validation is performed: malformed protobuf messages are discarded by the socket handler, and invalid peer indices or room IDs do not corrupt the hub state.

\paragraph{Cryptography.}
No cryptographic mechanisms are used.
All communication (HTTP and UDP) is unencrypted.
In a production deployment, TLS termination at the ingress level would protect client-to-hub communication, while mutual TLS (mTLS) via a service mesh could secure intra-cluster traffic.

\subsubsection{Room Server}

\paragraph{Authentication.}
There is minimal authentication at the Room tier. 
When a client connects, their first message must contain their \textit{Player ID}. 
The Room Server verifies this ID against the list of expected players provided by the Game Engine during the game startup phase. 
If the ID is already taken by another connected client, or if it does not match any expected player, the server rejects the connection. 

\paragraph{Authorization (Server-Side Validation).}
The most critical security mechanism in the Room Server is \textbf{State Authorization}. 
Clients are not authorized to dictate the game state. They cannot send "I am at coordinate (5,5)"; they can only send "I want to move UP". 
The Room Server validates every request against the game rules (e.g., checking for wall collisions) before applying it, neutralizing client-side manipulation (e.g., teleportation).

\paragraph{Cryptography.}
Like the Hub Server, the Room Server relies on unencrypted TCP streams. 
While vulnerable to packet sniffing, it is deemed acceptable for the purpose of this application.